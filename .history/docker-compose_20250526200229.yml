version: '3.8'

services:
  db:
    image: postgres:13-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=appuser
      - POSTGRES_PASSWORD=apppassword 
      - POSTGRES_DB=appdb
    ports:
      - "5433:5432" # Expose on 5433 on host to avoid conflict if local PG is on 5432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U appuser -d appdb"]
      interval: 10s
      timeout: 5s
      retries: 5

  backend:
    build:
      context: ./backend 
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      # Live code reloading for backend app development
      - ./backend/app:/app/app
      # Alembic config and migrations
      - ./backend/alembic:/app/alembic
      - ./backend/alembic.ini:/app/alembic.ini
      # ML model artifacts - this volume ensures the model trained by scheduler (inside container)
      # is available to the app and persists if the container is recreated.
      # The path /app/app/ml_model/artifacts inside the container is where the app will look.
      - ./backend/app/ml_model/artifacts:/app/app/ml_model/artifacts 
      # Potentially SQLite DB if used and needs persistence outside container (not primary for this setup)
      # - ./backend/app/db:/app/app/db
    environment:
      # --- Database ---
      - DATABASE_URL=postgresql://appuser:apppassword@db:5432/appdb
      # --- Python ---
      - PYTHONPATH=/app # Ensures 'app' module is found (e.g. app.main)
      # --- OpenAI ---
      - OPENAI_API_KEY=${OPENAI_API_KEY} # Pass through from host .env file or environment
      # --- ML Model Path (inside container) ---
      # This path MUST match where the application (ml_predictions_api.py) expects to find the model
      # and where the volume ./backend/app/ml_model/artifacts is mounted.
      - MODEL_PATH=/app/app/ml_model/artifacts/model.joblib 
      # --- Scheduler Settings (examples, can be overridden by host env or .env file) ---
      - ASSEMBLE_CRON_HOUR=${ASSEMBLE_CRON_HOUR:-1} # Default 1 AM UTC if not set
      - ASSEMBLE_CRON_MINUTE=${ASSEMBLE_CRON_MINUTE:-0}
      - TRAIN_CRON_HOUR=${TRAIN_CRON_HOUR:-2} # Default 2 AM UTC if not set
      - TRAIN_CRON_MINUTE=${TRAIN_CRON_MINUTE:-0}
      - STATS_UPDATE_CRON_HOUR=${STATS_UPDATE_CRON_HOUR:-0} # Default 00:30 AM UTC if not set
      - STATS_UPDATE_CRON_MINUTE=${STATS_UPDATE_CRON_MINUTE:-30}
      # --- Model Reload Endpoint (used by scheduler_setup.py to call back into the app) ---
      # Assuming the backend service is reachable as 'backend' from within Docker network,
      # but for HTTP calls from a subprocess within the same container, localhost is fine.
      - MODEL_RELOAD_URL=http://localhost:8000/ml/internal/reload_model
      - MODEL_RELOAD_SECRET=${MODEL_RELOAD_SECRET:-your-super-secret-key} # Use a strong secret
      # --- ML Prediction Endpoint (used by autobidder_service.py) ---
      # If autobidder runs in same container, localhost is fine. If separate, service name.
      - ML_PREDICTION_ENDPOINT_URL=http://localhost:8000/ml/autobid/predict_success_proba
      - ML_PROBABILITY_THRESHOLD=${ML_PROBABILITY_THRESHOLD:-0.5}
    depends_on:
      db:
        condition: service_healthy # Wait for db to be healthy
    # Command to run migrations then start the application
    # Note: alembic.ini might need its sqlalchemy.url updated if it's hardcoded to sqlite
    # For this command to work, alembic must be callable and correctly configured for PostgreSQL.
    command: >
      sh -c "
      echo 'Waiting for database to be ready...' &&
      while ! pg_isready -h db -p 5432 -U appuser -d appdb; do
        sleep 2;
      done;
      echo 'Database is ready. Running migrations...' &&
      alembic -c /app/alembic.ini upgrade head &&
      echo 'Migrations complete. Starting Uvicorn server...' &&
      uvicorn app.main:app --host 0.0.0.0 --port 8000
      "
      # Simpler command if migrations are handled manually or another way:
      # command: uvicorn app.main:app --host 0.0.0.0 --port 8000

volumes:
  postgres_data: # Defines the named volume for PostgreSQL data persistence
